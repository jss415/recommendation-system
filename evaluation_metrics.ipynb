{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import ndcg_score, dcg_score"
      ],
      "metadata": {
        "id": "tAF0j_LxNZMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wc1dwy2ot2u"
      },
      "outputs": [],
      "source": [
        "def precision_recall_fmeasure(predictions, threshold):\n",
        "    predicted_and_true_ratings = defaultdict(list)\n",
        "\n",
        "    for userId, _, true_rating, predicted_rating, _ in predictions:\n",
        "        predicted_and_true_ratings[userId].append((predicted_rating, true_rating))\n",
        "\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    \n",
        "    for userId, user_ratings in predicted_and_true_ratings.items():\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        num_relevant_items = sum((true_rating >= threshold) for (_, true_rating) in user_ratings)\n",
        "\n",
        "        num_recommended_items = sum((predicted_rating >= threshold) for (predicted_rating, _) in user_ratings[:10])\n",
        "\n",
        "        num_relevant_and_recommended_items = sum(((true_rating >= threshold) and (predicted_rating >= threshold))\n",
        "                              for (predicted_rating, true_rating) in user_ratings[:10])\n",
        "\n",
        "        current_user_precision = num_relevant_and_recommended_items / num_recommended_items if num_recommended_items != 0 else 0\n",
        "        current_user_recall = num_relevant_and_recommended_items / num_relevant_items if num_relevant_items != 0 else 0\n",
        "\n",
        "        precisions.append(current_user_precision)\n",
        "        recalls.append(current_user_recall)\n",
        "\n",
        "    precision = sum(precisions) / len(precisions)\n",
        "    recall = sum(recalls) / len(recalls)\n",
        "    f_measure = 2*precision*recall/(precision + recall)\n",
        "\n",
        "    return precision, recall, f_measure  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg(predictions):\n",
        "    ndcgs = []\n",
        "\n",
        "    predicted_and_true_ratings = defaultdict(list)\n",
        "    for userId, _, true_rating, predicted_rating, _ in predictions:\n",
        "        predicted_and_true_ratings[userId].append((predicted_rating, true_rating))\n",
        "    \n",
        "    for userId, user_ratings in predicted_and_true_ratings.items():\n",
        "        top_items = min(len(user_ratings), 10)\n",
        "    \n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        model_relevance = np.asarray([[round(x[0]) for x in user_ratings][:top_items]])\n",
        "        \n",
        "        user_ratings.sort(key=lambda x: x[1], reverse = True)\n",
        "        true_relevance = np.asarray([[round(x[1]) for x in user_ratings][:top_items]])\n",
        "         \n",
        "        try: \n",
        "            user_ndcg = ndcg_score(true_relevance, model_relevance)\n",
        "            ndcgs.append(user_ndcg)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    return sum(ndcgs)/len(ndcgs)"
      ],
      "metadata": {
        "id": "mkHYlDxE1AuM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}